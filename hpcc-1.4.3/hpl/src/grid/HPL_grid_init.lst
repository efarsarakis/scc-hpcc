%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                          S u m m a r y   R e p o r t
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Compilation
-----------
File     : /fs3/d59/d59/s1361615/isc14/scc-hpcc/hpcc-1.4.3/hpl/lib/arch/build/../../../src/grid/HPL_grid_init.c
Compiled : 2014-03-24  21:32:23
Compiler : Version 8.2.x.x
Ftnlx    : Version 8232 (libcif 82024)
Target   : x86-64
Command  : driver.cc -h cpu=ivybridge -h static -h network=aries
           -o ../../../src/grid/HPL_grid_init.o
           -c ../../../src/grid/HPL_grid_init.c -I ../../../include
           -I ../../../include/CrayX1 -D Add_ -D StringSunStyle
           -D F77_INTEGER=int -O 2 -h list=m -D LONG_IS_64BITS -h restrict=a
           -ibase-compiler /opt/cray/cce/8.2.1/CC/x86-64/compiler_include_base
           -isystem /opt/cray/cce/8.2.1/craylibs/x86-64/include
           -I /opt/gcc/4.4.4/snos/lib/gcc/x86_64-suse-linux/4.4.4/include
           -I /opt/gcc/4.4.4/snos/lib/gcc/x86_64-suse-linux/4.4.4/include-fixed
           -L /opt/cray/cce/8.2.1/CC/x86-64/lib/x86-64
           -W l,-rpath=/opt/cray/cce/8.2.1/CC/x86-64/lib/x86-64
           -L /opt/gcc/4.4.4/snos/lib64 -W l,-rpath=/opt/gcc/4.4.4/snos/lib64
           -L /opt/cray/cce/8.2.1/craylibs/x86-64
           -W l,-rpath=/opt/cray/cce/8.2.1/craylibs/x86-64 -lcraymath
           -lquadmath -lcraymp
           -I /opt/cray/rca/1.0.0-2.0500.41336.1.120.ari/include
           -I /opt/cray/alps/5.0.3-2.0500.8095.1.1.ari/include
           -I /opt/cray/xpmem/0.1-2.0500.41356.1.11.ari/include
           -I /opt/cray/gni-headers/3.0-1.0500.7161.11.4.ari/include
           -I /opt/cray/dmapp/6.0.1-1.0500.7263.9.31.ari/include
           -I /opt/cray/pmi/4.0.1-1.0000.9753.86.2.ari/include
           -I /opt/cray/ugni/5.0-1.0500.0.3.306.ari/include
           -I /opt/cray/udreg/2.3.2-1.0500.6756.2.10.ari/include
           -I /opt/cray-hss-devel/7.0.0/include
           -I /opt/cray/krca/1.0.0-2.0500.41867.2.75.ari/include
           -L /opt/cray/rca/1.0.0-2.0500.41336.1.120.ari/lib64
           -L /opt/cray/alps/5.0.3-2.0500.8095.1.1.ari/lib64
           -L /opt/cray/xpmem/0.1-2.0500.41356.1.11.ari/lib64
           -L /opt/cray/dmapp/6.0.1-1.0500.7263.9.31.ari/lib64
           -L /opt/cray/pmi/4.0.1-1.0000.9753.86.2.ari/lib64
           -L /opt/cray/ugni/5.0-1.0500.0.3.306.ari/lib64
           -L /opt/cray/udreg/2.3.2-1.0500.6756.2.10.ari/lib64
           -I /opt/cray/mpt/6.1.1/gni/mpich2-cray/81/include
           -I /opt/cray/libsci/12.1.2/CRAY/81/sandybridge/include
           -I /opt/fftw/3.3.0.4/sandybridge/include
           -I /opt/cray/rca/1.0.0-2.0500.41336.1.120.ari/include
           -I /opt/cray/alps/5.0.3-2.0500.8095.1.1.ari/include
           -I /opt/cray/xpmem/0.1-2.0500.41356.1.11.ari/include
           -I /opt/cray/gni-headers/3.0-1.0500.7161.11.4.ari/include
           -I /opt/cray/dmapp/6.0.1-1.0500.7263.9.31.ari/include
           -I /opt/cray/pmi/4.0.1-1.0000.9753.86.2.ari/include
           -I /opt/cray/ugni/5.0-1.0500.0.3.306.ari/include
           -I /opt/cray/udreg/2.3.2-1.0500.6756.2.10.ari/include
           -I /opt/cray-hss-devel/7.0.0/include
           -I /opt/cray/krca/1.0.0-2.0500.41867.2.75.ari/include

clx report
------------
Source   : /fs3/d59/d59/s1361615/isc14/scc-hpcc/hpcc-1.4.3/hpl/lib/arch/build/../../../src/grid/HPL_grid_init.c
Date     : 03/24/2014  21:32:25


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                          S o u r c e   L i s t i n g
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


     %%%    L o o p m a r k   L e g e n d    %%%

     Primary Loop Type        Modifiers
     ------- ---- ----        ---------
     A - Pattern matched      a - atomic memory operation
                              b - blocked
     C - Collapsed            c - conditional and/or computed
     D - Deleted               
     E - Cloned               f - fused
     G - Accelerated          g - partitioned
     I - Inlined              i - interchanged
     M - Multithreaded        m - partitioned
                              n - non-blocking remote transfer
                              p - partial
                              r - unrolled
                              s - shortloop
     V - Vectorized           w - unwound

     + - More messages listed at end of listing
     ------------------------------------------


    1.          /* 
    2.           * -- High Performance Computing Linpack Benchmark (HPL)                
    3.           *    HPL - 2.0 - September 10, 2008                          
    4.           *    Antoine P. Petitet                                                
    5.           *    University of Tennessee, Knoxville                                
    6.           *    Innovative Computing Laboratory                                 
    7.           *    (C) Copyright 2000-2008 All Rights Reserved                       
    8.           *                                                                      
    9.           * -- Copyright notice and Licensing terms:                             
   10.           *                                                                      
   11.           * Redistribution  and  use in  source and binary forms, with or without
   12.           * modification, are  permitted provided  that the following  conditions
   13.           * are met:                                                             
   14.           *                                                                      
   15.           * 1. Redistributions  of  source  code  must retain the above copyright
   16.           * notice, this list of conditions and the following disclaimer.        
   17.           *                                                                      
   18.           * 2. Redistributions in binary form must reproduce  the above copyright
   19.           * notice, this list of conditions,  and the following disclaimer in the
   20.           * documentation and/or other materials provided with the distribution. 
   21.           *                                                                      
   22.           * 3. All  advertising  materials  mentioning  features  or  use of this
   23.           * software must display the following acknowledgement:                 
   24.           * This  product  includes  software  developed  at  the  University  of
   25.           * Tennessee, Knoxville, Innovative Computing Laboratory.             
   26.           *                                                                      
   27.           * 4. The name of the  University,  the name of the  Laboratory,  or the
   28.           * names  of  its  contributors  may  not  be used to endorse or promote
   29.           * products  derived   from   this  software  without  specific  written
   30.           * permission.                                                          
   31.           *                                                                      
   32.           * -- Disclaimer:                                                       
   33.           *                                                                      
   34.           * THIS  SOFTWARE  IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
   35.           * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES,  INCLUDING,  BUT NOT
   36.           * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
   37.           * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE UNIVERSITY
   38.           * OR  CONTRIBUTORS  BE  LIABLE FOR ANY  DIRECT,  INDIRECT,  INCIDENTAL,
   39.           * SPECIAL,  EXEMPLARY,  OR  CONSEQUENTIAL DAMAGES  (INCLUDING,  BUT NOT
   40.           * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
   41.           * DATA OR PROFITS; OR BUSINESS INTERRUPTION)  HOWEVER CAUSED AND ON ANY
   42.           * THEORY OF LIABILITY, WHETHER IN CONTRACT,  STRICT LIABILITY,  OR TORT
   43.           * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
   44.           * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. 
   45.           * ---------------------------------------------------------------------
   46.           */ 
   47.          /*
   48.           * Include files
   49.           */
   50.          #include "hpl.h"
   51.          
   52.          #ifdef HPL_STDC_HEADERS
   53.          int HPL_grid_init
   54.          (
   55.             MPI_Comm                         COMM,
   56.             const HPL_T_ORDER                ORDER,
   57.             const int                        NPROW,
   58.             const int                        NPCOL,
   59.             HPL_T_grid *                     GRID
   60.          )
   61.          #else
   62.          int HPL_grid_init
   63.          ( COMM, ORDER, NPROW, NPCOL, GRID )
   64.             MPI_Comm                         COMM;
   65.             const HPL_T_ORDER                ORDER;
   66.             const int                        NPROW;
   67.             const int                        NPCOL;
   68.             HPL_T_grid *                     GRID;
   69.          #endif
   70.          {
   71.          /* 
   72.           * Purpose
   73.           * =======
   74.           *
   75.           * HPL_grid_init creates a NPROW x NPCOL  process  grid using column- or
   76.           * row-major ordering from an initial collection of processes identified
   77.           * by an  MPI  communicator.  Successful  completion is indicated by the
   78.           * returned error code MPI_SUCCESS.  Other error codes depend on the MPI
   79.           * implementation. The coordinates of processes that are not part of the
   80.           * grid are set to values outside of [0..NPROW) x [0..NPCOL).
   81.           *
   82.           * Arguments
   83.           * =========
   84.           *
   85.           * COMM    (global/local input)          MPI_Comm
   86.           *         On entry,  COMM  is  the  MPI  communicator  identifying  the
   87.           *         initial  collection  of  processes out of which  the  grid is
   88.           *         formed.
   89.           *
   90.           * ORDER   (global input)                const HPL_T_ORDER
   91.           *         On entry, ORDER specifies how the processes should be ordered
   92.           *         in the grid as follows:
   93.           *            ORDER = HPL_ROW_MAJOR    row-major    ordering;
   94.           *            ORDER = HPL_COLUMN_MAJOR column-major ordering;
   95.           *
   96.           * NPROW   (global input)                const int
   97.           *         On entry,  NPROW  specifies the number of process rows in the
   98.           *         grid to be created. NPROW must be at least one.
   99.           *
  100.           * NPCOL   (global input)                const int
  101.           *         On entry,  NPCOL  specifies  the number of process columns in
  102.           *         the grid to be created. NPCOL must be at least one.
  103.           *
  104.           * GRID    (local input/output)          HPL_T_grid *
  105.           *         On entry,  GRID  points  to the data structure containing the
  106.           *         process grid information to be initialized.
  107.           *
  108.           * ---------------------------------------------------------------------
  109.           */ 
  110.          /*
  111.           * .. Local Variables ..
  112.           */
  113.             int                        hdim, hplerr=MPI_SUCCESS, ierr, ip2, k,
  114.                                        mask, mycol, myrow, nprocs, rank, size;
  115.          /* ..
  116.           * .. Executable Statements ..
  117.           */
  118.  +          MPI_Comm_rank( COMM, &rank ); MPI_Comm_size( COMM, &size );
  119.          /*
  120.           * Abort if illegal process grid
  121.           */
  122.             nprocs = NPROW * NPCOL;
  123.             if( ( nprocs > size ) || ( NPROW < 1 ) || ( NPCOL < 1 ) )
  124.  +          { HPL_pabort( __LINE__, "HPL_grid_init", "Illegal Grid" ); }
  125.          /*
  126.           * Row- or column-major ordering of the processes
  127.           */
  128.             if( ORDER == HPL_ROW_MAJOR )
  129.             {
  130.                GRID->order = HPL_ROW_MAJOR;
  131.                myrow = rank / NPCOL; mycol = rank - myrow * NPCOL;
  132.             }
  133.             else
  134.             {
  135.                GRID->order = HPL_COLUMN_MAJOR;
  136.                mycol = rank / NPROW; myrow = rank - mycol * NPROW;
  137.             }
  138.             GRID->iam   = rank;  GRID->myrow = myrow; GRID->mycol  = mycol;
  139.             GRID->nprow = NPROW; GRID->npcol = NPCOL; GRID->nprocs = nprocs;
  140.          /*
  141.           * row_ip2   : largest power of two <= nprow;
  142.           * row_hdim  : row_ip2 procs hypercube dim;
  143.           * row_ip2m1 : largest power of two <= nprow-1;
  144.           * row_mask  : row_ip2m1 procs hypercube mask;
  145.           */
  146.             hdim = 0; ip2 = 1; k = NPROW;
  147.  + 1--<>    while( k > 1 ) { k >>= 1; ip2 <<= 1; hdim++; }
  148.             GRID->row_ip2 = ip2; GRID->row_hdim = hdim; 
  149.          
  150.             mask = ip2 = 1;    k = NPROW - 1;
  151.  + 1--<>    while( k > 1 ) { k >>= 1; ip2 <<= 1; mask <<= 1; mask++; }
  152.             GRID->row_ip2m1 = ip2; GRID->row_mask = mask; 
  153.          /*
  154.           * col_ip2   : largest power of two <= npcol;
  155.           * col_hdim  : col_ip2 procs hypercube dim;
  156.           * col_ip2m1 : largest power of two <= npcol-1;
  157.           * col_mask  : col_ip2m1 procs hypercube mask;
  158.           */
  159.             hdim = 0; ip2 = 1; k = NPCOL;
  160.  + 1--<>    while( k > 1 ) { k >>= 1; ip2 <<= 1; hdim++; }
  161.             GRID->col_ip2 = ip2; GRID->col_hdim = hdim; 
  162.          
  163.             mask = ip2 = 1;    k = NPCOL - 1;
  164.  + 1--<>    while( k > 1 ) { k >>= 1; ip2 <<= 1; mask <<= 1; mask++; }
  165.             GRID->col_ip2m1 = ip2; GRID->col_mask = mask; 
  166.          /*
  167.           * All communicator, leave if I am not part of this grid. Creation of the
  168.           * row- and column communicators.
  169.           */
  170.  +          ierr = MPI_Comm_split( COMM, ( rank < nprocs ? 0 : MPI_UNDEFINED ),
  171.                                    rank, &(GRID->all_comm) );
  172.             if( GRID->all_comm == MPI_COMM_NULL ) return( ierr );
  173.          
  174.  +          ierr = MPI_Comm_split( GRID->all_comm, myrow, mycol, &(GRID->row_comm) );
  175.             if( ierr != MPI_SUCCESS ) hplerr = ierr;
  176.          
  177.  +          ierr = MPI_Comm_split( GRID->all_comm, mycol, myrow, &(GRID->col_comm) );
  178.             if( ierr != MPI_SUCCESS ) hplerr = ierr;
  179.          
  180.             return( hplerr );
  181.          /*
  182.           * End of HPL_grid_init
  183.           */
  184.          }

CC-3021 CC: IPA File = HPL_grid_init.c, Line = 118 
  "MPI_Comm_rank" (called from "HPL_grid_init") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = HPL_grid_init.c, Line = 118 
  "MPI_Comm_size" (called from "HPL_grid_init") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = HPL_grid_init.c, Line = 124 
  "HPL_pabort" (called from "HPL_grid_init") was not inlined because the compiler was unable to locate the routine.

CC-6254 CC: VECTOR File = HPL_grid_init.c, Line = 147 
  A loop was not vectorized because a recurrence was found on "ip2" at line 147.

CC-6254 CC: VECTOR File = HPL_grid_init.c, Line = 151 
  A loop was not vectorized because a recurrence was found on "ip2" at line 151.

CC-6254 CC: VECTOR File = HPL_grid_init.c, Line = 160 
  A loop was not vectorized because a recurrence was found on "ip2" at line 160.

CC-6254 CC: VECTOR File = HPL_grid_init.c, Line = 164 
  A loop was not vectorized because a recurrence was found on "ip2" at line 164.

CC-3021 CC: IPA File = HPL_grid_init.c, Line = 170 
  "MPI_Comm_split" (called from "HPL_grid_init") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = HPL_grid_init.c, Line = 174 
  "MPI_Comm_split" (called from "HPL_grid_init") was not inlined because the compiler was unable to locate the routine.

CC-3021 CC: IPA File = HPL_grid_init.c, Line = 177 
  "MPI_Comm_split" (called from "HPL_grid_init") was not inlined because the compiler was unable to locate the routine.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
